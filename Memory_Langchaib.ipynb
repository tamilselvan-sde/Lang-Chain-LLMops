{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# --- Load envs (expects OPENAI_API_KEY and optional MLFLOW_TRACKING_URI) ---\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY missing in environment/.env\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer Memory\n",
    "ConversationBufferMemory keeps a list of chat messages in a buffer and passes those into the prompt template.\n",
    "\n",
    "![Buffer Memory](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sUkvbURF633MXJyX.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/9z6rn3xs13x2b0bkcf9ljwbw0000gp/T/ipykernel_7134/1027041917.py:12: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n",
      "/var/folders/g4/9z6rn3xs13x2b0bkcf9ljwbw0000gp/T/ipykernel_7134/1027041917.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  print(conversation.run(\"Hi, my name is Tamil.\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Tamil.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello, Tamil! It's great to meet you! How are you doing today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Tamil.\n",
      "AI: Hello, Tamil! It's great to meet you! How are you doing today?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Your name is Tamil! It's nice to connect with you. How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory #(Type -1 )\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(conversation.run(\"Hi, my name is Tamil.\"))\n",
    "print(conversation.run(\"What is my name?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/9z6rn3xs13x2b0bkcf9ljwbw0000gp/T/ipykernel_7134/1463898629.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/9z6rn3xs13x2b0bkcf9ljwbw0000gp/T/ipykernel_7134/2838645039.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  conversation({\"question\": \"hi\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: My name is Tamil and I have 5 cows.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is Tamil and I have 5 cows.',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='My name is Tamil and I have 5 cows.'),\n",
       "  AIMessage(content='Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?')],\n",
       " 'text': 'Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"My name is Tamil and I have 5 cows.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: My name is Tamil and I have 5 cows.\n",
      "AI: Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?\n",
      "Human: how many cows i have?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'how many cows i have?',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='My name is Tamil and I have 5 cows.'),\n",
       "  AIMessage(content='Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?'),\n",
       "  HumanMessage(content='how many cows i have?'),\n",
       "  AIMessage(content=\"You mentioned that you have 5 cows. Is there anything specific you'd like to share or ask about them?\")],\n",
       " 'text': \"You mentioned that you have 5 cows. Is there anything specific you'd like to share or ask about them?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"how many cows i have?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: My name is Tamil and I have 5 cows.\n",
      "AI: Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?\n",
      "Human: how many cows i have?\n",
      "AI: You mentioned that you have 5 cows. Is there anything specific you'd like to share or ask about them?\n",
      "Human: Do you remember my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Do you remember my name?',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='My name is Tamil and I have 5 cows.'),\n",
       "  AIMessage(content='Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?'),\n",
       "  HumanMessage(content='how many cows i have?'),\n",
       "  AIMessage(content=\"You mentioned that you have 5 cows. Is there anything specific you'd like to share or ask about them?\"),\n",
       "  HumanMessage(content='Do you remember my name?'),\n",
       "  AIMessage(content='Yes, your name is Tamil! How can I help you today, Tamil?')],\n",
       " 'text': 'Yes, your name is Tamil! How can I help you today, Tamil?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"Do you remember my name?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi'), AIMessage(content='Hello! How can I assist you today?'), HumanMessage(content='My name is Tamil and I have 5 cows.'), AIMessage(content='Hi Tamil! That’s great to hear! Cows can be wonderful animals. Do you have any specific questions or topics related to your cows that you’d like to discuss?'), HumanMessage(content='how many cows i have?'), AIMessage(content=\"You mentioned that you have 5 cows. Is there anything specific you'd like to share or ask about them?\"), HumanMessage(content='Do you remember my name?'), AIMessage(content='Yes, your name is Tamil! How can I help you today, Tamil?')]\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Buffer Window Memory\n",
    "\n",
    "Similar to the previous one, but you can limit the number of conversational exchanges stored in memory. For example, you can set it so it only remembers the last 3 questions and answers of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[User 1]: Hi, I'm building a pizza chatbot for our shop.\n",
      "[Bot  1]: That sounds great! How can I assist you with your pizza chatbot? Do you need help with features, conversation flow, or something else?\n",
      "\n",
      "[User 2]: I like spicy paneer toppings.\n",
      "[Bot  2]: Spicy paneer is a delicious choice! Would you like to include it as a topping option in your chatbot? If so, I can help you with how to present it or incorporate it into the conversation flow.\n",
      "\n",
      "[User 3]: What would be a fun brand name?\n",
      "[Bot  3]: Here are some fun brand name ideas for your pizza shop:\n",
      "\n",
      "1. **Paneer Paradise**\n",
      "2. **Spicy Slice**\n",
      "3. **Cheesy Chaat**\n",
      "4. **Paneer Pizzazz**\n",
      "5. **Zesty Za**\n",
      "6. **Pizza Fiesta**\n",
      "7. **Spice & Slice**\n",
      "8. **Paneer Pizzicato**\n",
      "9. **Fiery Crust**\n",
      "10. **Topping Tango**\n",
      "\n",
      "Feel free to mix and match or modify them to suit your style!\n",
      "\n",
      "[User 4]: Remind me what topping I like, and suggest 2 names.\n",
      "[Bot  4]: You like spicy paneer toppings! Here are two brand name suggestions based on that:\n",
      "\n",
      "1. **Spicy Paneer Pizzeria**\n",
      "2. **Paneer Heat Pizza Co.**\n",
      "\n",
      "Let me know if you need more options!\n",
      "\n",
      "[User 5]: Also, what did I say first about my project?\n",
      "[Bot  5]: You mentioned that you like spicy paneer toppings. If you need more information or assistance with your project, feel free to ask!\n",
      "\n",
      "--- Memory window contents (history) ---\n",
      "Human  : Hi, I'm building a pizza chatbot for our shop.\n",
      "AI     : That sounds great! How can I assist you with your pizza chatbot? Do you need help with features, conversation flow, or something else?\n",
      "Human  : I like spicy paneer toppings.\n",
      "AI     : Spicy paneer is a delicious choice! Would you like to include it as a topping option in your chatbot? If so, I can help you with how to present it or incorporate it into the conversation flow.\n",
      "Human  : What would be a fun brand name?\n",
      "AI     : Here are some fun brand name ideas for your pizza shop:\n",
      "\n",
      "1. **Paneer Paradise**\n",
      "2. **Spicy Slice**\n",
      "3. **Cheesy Chaat**\n",
      "4. **Paneer Pizzazz**\n",
      "5. **Zesty Za**\n",
      "6. **Pizza Fiesta**\n",
      "7. **Spice & Slice**\n",
      "8. **Paneer Pizzicato**\n",
      "9. **Fiery Crust**\n",
      "10. **Topping Tango**\n",
      "\n",
      "Feel free to mix and match or modify them to suit your style!\n",
      "Human  : Remind me what topping I like, and suggest 2 names.\n",
      "AI     : You like spicy paneer toppings! Here are two brand name suggestions based on that:\n",
      "\n",
      "1. **Spicy Paneer Pizzeria**\n",
      "2. **Paneer Heat Pizza Co.**\n",
      "\n",
      "Let me know if you need more options!\n",
      "Human  : Also, what did I say first about my project?\n",
      "AI     : You mentioned that you like spicy paneer toppings. If you need more information or assistance with your project, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1) Set your key: export OPENAI_API_KEY=sk-... (Linux/macOS) or set in env variables on Windows\n",
    "#    Or uncomment next line and paste directly (not recommended)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "def make_chain(k_window: int = 3):\n",
    "    \"\"\"\n",
    "    Build a chat chain that only remembers the last k exchanges.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "    # Memory that only keeps the last k turns\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        k=k_window,\n",
    "        memory_key=\"history\",\n",
    "        return_messages=True,   # important for ChatPromptTemplate\n",
    "    )\n",
    "\n",
    "    # Prompt that includes the rolling \"history\" plus the new user message\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a concise assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=False)\n",
    "    return chain, memory\n",
    "\n",
    "def demo():\n",
    "    chain, memory = make_chain(k_window=3)\n",
    "\n",
    "    user_messages = [\n",
    "        \"Hi, I'm building a pizza chatbot for our shop.\",\n",
    "        \"I like spicy paneer toppings.\",\n",
    "        \"What would be a fun brand name?\",\n",
    "        \"Remind me what topping I like, and suggest 2 names.\",\n",
    "        \"Also, what did I say first about my project?\",\n",
    "    ]\n",
    "\n",
    "    for i, msg in enumerate(user_messages, start=1):\n",
    "        print(f\"\\n[User {i}]: {msg}\")\n",
    "        reply = chain.run(input=msg)  # memory is applied automatically\n",
    "        print(f\"[Bot  {i}]: {reply}\")\n",
    "\n",
    "    # Inspect what's currently in memory (only the last k exchanges remain)\n",
    "    print(\"\\n--- Memory window contents (history) ---\")\n",
    "    for m in memory.chat_memory.messages:\n",
    "        role = type(m).__name__.replace(\"Message\", \"\")\n",
    "        print(f\"{role:7s}: {getattr(m, 'content', '')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Message History\n",
    "Save list of chat messages and then fetch them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--id-col 'id' not found. CSV columns: ['name', 'age', 'mark']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 134\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedded and stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_added\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new row(s). Done.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[43membed_csv_incremental\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPERSIST_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mID_COL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEXT_COLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBED_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 81\u001b[0m, in \u001b[0;36membed_csv_incremental\u001b[0;34m(csv_path, collection_name, persist_dir, id_col, text_cols, embedding_model, batch_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Load CSV and prepare id\u001b[39;00m\n\u001b[1;32m     80\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m---> 81\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_get_or_make_row_id_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Init Chroma\u001b[39;00m\n\u001b[1;32m     84\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39mpersist_dir)\n",
      "Cell \u001b[0;32mIn[37], line 48\u001b[0m, in \u001b[0;36m_get_or_make_row_id_column\u001b[0;34m(df, id_col)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m id_col:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m id_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--id-col \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. CSV columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     50\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_row_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[id_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: --id-col 'id' not found. CSV columns: ['name', 'age', 'mark']"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Incremental CSV Embedding (LangChain + OpenAI + Chroma)\n",
    "# - First run: embeds all rows from data.csv\n",
    "# - Later: only embeds rows with new IDs (no full re-embed)\n",
    "# \n",
    "# Adjust CONFIG below and run this single cell.\n",
    "\n",
    "# %%capture\n",
    "# !pip install -q --upgrade langchain langchain-openai chromadb pandas tqdm\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# =========================\n",
    "# CONFIG — EDIT THESE\n",
    "# =========================\n",
    "CSV_PATH = \"data.csv\"                # your CSV file\n",
    "ID_COL = \"id\"                        # unique id column in CSV; set to None to use row index\n",
    "TEXT_COLS = [\"title\", \"body\"]        # which columns to embed; set to None to embed all non-id columns\n",
    "COLLECTION_NAME = \"csv_embeddings\"   # Chroma collection name\n",
    "PERSIST_DIR = \"./chroma_db\"          # directory to persist Chroma\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Option A: use environment variable (recommended)\n",
    "# export OPENAI_API_KEY=sk-...\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Option B: paste here if not using env var (not recommended)\n",
    "# OPENAI_API_KEY = \"sk-...\"\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _require_key():\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set. Export it or set OPENAI_API_KEY variable above.\")\n",
    "\n",
    "def _get_or_make_row_id_column(df: pd.DataFrame, id_col: Optional[str]) -> pd.DataFrame:\n",
    "    if id_col:\n",
    "        if id_col not in df.columns:\n",
    "            raise ValueError(f\"--id-col '{id_col}' not found. CSV columns: {list(df.columns)}\")\n",
    "        df = df.copy()\n",
    "        df[\"_row_id\"] = df[id_col].astype(str)\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df[\"_row_id\"] = df.index.astype(str)\n",
    "    return df\n",
    "\n",
    "def _build_text(row: pd.Series, text_cols: Optional[List[str]]) -> str:\n",
    "    if text_cols:\n",
    "        vals = [str(row[c]) for c in text_cols if c in row.index]\n",
    "        return \" | \".join(vals)\n",
    "    # default: join everything except _row_id and id_col (if present)\n",
    "    excluded = {\"_row_id\"}\n",
    "    return \" | \".join(f\"{c}: {row[c]}\" for c in row.index if c not in excluded)\n",
    "\n",
    "# =========================\n",
    "# Core incremental embed\n",
    "# =========================\n",
    "def embed_csv_incremental(\n",
    "    csv_path: str,\n",
    "    collection_name: str,\n",
    "    persist_dir: str,\n",
    "    id_col: Optional[str],\n",
    "    text_cols: Optional[List[str]],\n",
    "    embedding_model: str,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    _require_key()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "    # Load CSV and prepare idfig in the cell\n",
    "ID_COL = None\n",
    "TEXT_COLS = [\"name\", \"age\", \"mark\"]   # or None to use all\n",
    "\n",
    "    df = _get_or_make_row_id_column(df, id_col=id_col)\n",
    "\n",
    "    # Init Chroma\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # Existing IDs in the vector store\n",
    "    existing_ids = set(collection.get(include=[], limit=10_000).get(\"ids\", []))\n",
    "\n",
    "    # New rows mask\n",
    "    all_ids = df[\"_row_id\"].astype(str).tolist()\n",
    "    new_mask = [rid not in existing_ids for rid in all_ids]\n",
    "    df_new = df[new_mask]\n",
    "\n",
    "    if df_new.empty:\n",
    "        print(\"No new rows to embed. (Everything up to date.)\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(df_new)} new row(s) to embed out of {len(df)} total.\")\n",
    "\n",
    "    # Embedder\n",
    "    embedder = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "    ids_batch, docs_batch, metas_batch = [], [], []\n",
    "    total_added = 0\n",
    "\n",
    "    for _, row in tqdm(df_new.iterrows(), total=len(df_new), desc=\"Embedding\"):\n",
    "        rid = str(row[\"_row_id\"])\n",
    "        text = _build_text(row, text_cols=text_cols)\n",
    "\n",
    "        ids_batch.append(rid)\n",
    "        docs_batch.append(text)\n",
    "        # add original columns as metadata (optional; strings only)\n",
    "        meta = {k: (None if pd.isna(v) else str(v)) for k, v in row.drop(labels=[\"_row_id\"]).to_dict().items()}\n",
    "        metas_batch.append(meta)\n",
    "\n",
    "        if len(ids_batch) >= batch_size:\n",
    "            embs = embedder.embed_documents(docs_batch)\n",
    "            collection.add(ids=ids_batch, documents=docs_batch, embeddings=embs, metadatas=metas_batch)\n",
    "            total_added += len(ids_batch)\n",
    "            ids_batch, docs_batch, metas_batch = [], [], []\n",
    "\n",
    "    # Flush remainder\n",
    "    if ids_batch:\n",
    "        embs = embedder.embed_documents(docs_batch)\n",
    "        collection.add(ids=ids_batch, documents=docs_batch, embeddings=embs, metadatas=metas_batch)\n",
    "        total_added += len(ids_batch)\n",
    "\n",
    "    print(f\"Embedded and stored {total_added} new row(s). Done.\")\n",
    "\n",
    "# =========================\n",
    "# Run\n",
    "# =========================\n",
    "embed_csv_incremental(\n",
    "    csv_path=CSV_PATH,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_dir=PERSIST_DIR,\n",
    "    id_col=ID_COL,\n",
    "    text_cols=TEXT_COLS,\n",
    "    embedding_model=EMBED_MODEL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
