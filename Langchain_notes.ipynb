{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Import userdata (where you stored your Colab secret)\n",
        "from google.colab import userdata\n",
        "\n",
        "# 2. Get your API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 3. Set it as environment variable so LangChain can detect it\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GBFg65TGqBFC"
      },
      "id": "GBFg65TGqBFC",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # works for gpt-4o, gpt-3.5-turbo, etc.\n",
        "    temperature=0.2,\n",
        "    max_tokens=50\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Explain LangChain in simple terms.\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ta2jEHqM5G",
        "outputId": "61a0b491-028a-450e-a674-20b03dddd4e6"
      },
      "id": "m0ta2jEHqM5G",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a framework designed to help developers build applications that use language models, like those from OpenAI. It simplifies the process of creating complex applications that can understand and generate human-like text.\n",
            "\n",
            "Here’s a simple breakdown of what LangChain does:\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion model\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llmModel = OpenAI()"
      ],
      "metadata": {
        "id": "vpAdgsHJqZV4"
      },
      "id": "vpAdgsHJqZV4",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response  = llmModel.invoke(\"Tell me one fun fact about the Kennedy family.\")\n"
      ],
      "metadata": {
        "id": "feRntFwAqdWq"
      },
      "id": "feRntFwAqdWq",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgdX0FMXq_WJ",
        "outputId": "3c6bd4ff-92b1-49fe-8dbe-41f364ece49c"
      },
      "id": "pgdX0FMXq_WJ",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "One fun fact about the Kennedy family is that John F. Kennedy, the 35th President of the United States, had a pet pony named Macaroni who lived on the White House lawn. Macaroni was a gift from a family friend and quickly became a favorite of the Kennedy children.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llmModel.stream(\n",
        "    \"Tell me one fun fact about the Kennedy family.i need detail response\"\n",
        "):\n",
        "    print(chunk, end=\"\", flush= True) # forces Python to immediately push the text to the terminal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E7cifSxreXk",
        "outputId": "fe248705-9321-4c30-e4a4-dbff1151be4d"
      },
      "id": "_E7cifSxreXk",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "One fun fact about the Kennedy family is that they were known for their love of sailing. President John F. Kennedy and his siblings were all avid sailors and spent much of their childhood on the water. The family owned several boats, including a 25-foot sloop named the Victura, which was a gift from their father, Joseph P. Kennedy. The Kennedy siblings often competed against each other in sailing races and even participated in the prestigious Newport Bermuda Race. Their love of sailing continued into adulthood, with President Kennedy famously hosting a meeting with Soviet leader Nikita Khrushchev on his yacht, the Honey Fitz. Even after the tragic death of President Kennedy, the family continued to sail and the tradition was passed down to future generations. Today, the Kennedy Compound in Hyannis Port, Massachusetts is still a popular spot for the family to gather and enjoy their love of sailing."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "creativeLlmModel = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "zzq8JroasSm_"
      },
      "id": "zzq8JroasSm_",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response  = creativeLlmModel.invoke(\"Tell me one fun fact about the Kennedy family.\")"
      ],
      "metadata": {
        "id": "cIsk3dYasXnC"
      },
      "id": "cIsk3dYasXnC",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "WqKBlhEksc6X",
        "outputId": "2dede4bc-b1e6-467a-a597-469779bec668"
      },
      "id": "WqKBlhEksc6X",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nOne fun fact about the Kennedy family is that President John F. Kennedy had a pet pony named Macaroni that lived at the White House. The pony even had its own specially designed stable and would often be seen being ridden around the grounds by JFK's children, Caroline and John Jr.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llmModel.invoke(\n",
        "    \"Write a short 5 line poem about JFK\"\n",
        ")"
      ],
      "metadata": {
        "id": "e6Ns5FhIse7L"
      },
      "id": "e6Ns5FhIse7L",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M1Wnz1Kskxf",
        "outputId": "9b6b1bd0-0716-4fa4-e853-ecd48a664882"
      },
      "id": "7M1Wnz1Kskxf",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "JFK, a president bold and wise,\n",
            "With dreams of unity and skies,\n",
            "His words inspired a nation's heart,\n",
            "Till tragedy tore us apart.\n",
            "But his legacy will never die, JFK forever in our minds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcsO87B-snUF"
      },
      "id": "TcsO87B-snUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Model"
      ],
      "metadata": {
        "id": "cW9GNAffsxZc"
      },
      "id": "cW9GNAffsxZc"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ],
      "metadata": {
        "id": "G5x3aAJgsyW-"
      },
      "id": "G5x3aAJgsyW-",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
        "    (\"human\", \"Tell me one curious thing about JFK.\"),\n",
        "]\n",
        "response = chatModel.invoke(messages)"
      ],
      "metadata": {
        "id": "m7TWI0j1s0XK"
      },
      "id": "m7TWI0j1s0XK",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAU5DQ3Xs3GW",
        "outputId": "5064ea6e-bad0-45a9-cffc-05dc6875f00d"
      },
      "id": "GAU5DQ3Xs3GW",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One curious thing about JFK is that he was the first president to hold a press conference that was broadcast live on television. This happened on January 25, 1961, and it set a precedent for future presidents to directly address the American public through this medium. Kennedy understood the power of television and used it effectively to communicate his message and connect with the American people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.response_metadata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWm7D7COs6uB",
        "outputId": "2fa48c95-abdb-4dc9-ea06-391210792651"
      },
      "id": "zWm7D7COs6uB",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 74,\n",
              "  'prompt_tokens': 29,\n",
              "  'total_tokens': 103,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'model_name': 'gpt-3.5-turbo-0125',\n",
              " 'system_fingerprint': None,\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l2M1vfFtCxz",
        "outputId": "f1150d66-ec4c-4d28-bc61-105defdc906e"
      },
      "id": "4l2M1vfFtCxz",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'AIMessage',\n",
              " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
              " 'type': 'object',\n",
              " 'properties': {'content': {'title': 'Content',\n",
              "   'anyOf': [{'type': 'string'},\n",
              "    {'type': 'array',\n",
              "     'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
              "  'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "  'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "  'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'},\n",
              "  'name': {'title': 'Name', 'type': 'string'},\n",
              "  'id': {'title': 'Id', 'type': 'string'},\n",
              "  'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
              "  'tool_calls': {'title': 'Tool Calls',\n",
              "   'default': [],\n",
              "   'type': 'array',\n",
              "   'items': {'$ref': '#/definitions/ToolCall'}},\n",
              "  'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
              "   'default': [],\n",
              "   'type': 'array',\n",
              "   'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
              "  'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
              " 'required': ['content'],\n",
              " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
              "   'type': 'object',\n",
              "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
              "    'args': {'title': 'Args', 'type': 'object'},\n",
              "    'id': {'title': 'Id', 'type': 'string'},\n",
              "    'type': {'title': 'Type', 'enum': ['tool_call'], 'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id']},\n",
              "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
              "   'type': 'object',\n",
              "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
              "    'args': {'title': 'Args', 'type': 'string'},\n",
              "    'id': {'title': 'Id', 'type': 'string'},\n",
              "    'error': {'title': 'Error', 'type': 'string'},\n",
              "    'type': {'title': 'Type',\n",
              "     'enum': ['invalid_tool_call'],\n",
              "     'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id', 'error']},\n",
              "  'UsageMetadata': {'title': 'UsageMetadata',\n",
              "   'type': 'object',\n",
              "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
              "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
              "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
              "   'required': ['input_tokens', 'output_tokens', 'total_tokens']}}}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Where you see this:\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Write this instead:\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "nGjrYwu_tYY2"
      },
      "id": "nGjrYwu_tYY2",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Old langchain"
      ],
      "metadata": {
        "id": "jhGWVhMhtxnc"
      },
      "id": "jhGWVhMhtxnc"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n"
      ],
      "metadata": {
        "id": "dld84__Utyu4"
      },
      "id": "dld84__Utyu4",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are an historian expert on the Kennedy Family.\"),\n",
        "    HumanMessage(content=\"How many children had Joseph P. Kennedy?\"),\n",
        "]\n",
        "\n",
        "response = chatModel.invoke(messages)"
      ],
      "metadata": {
        "id": "TWXbFLp4t0dU"
      },
      "id": "TWXbFLp4t0dU",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uKDvlS-ut2iJ",
        "outputId": "485168e2-9bb7-4b24-8e15-b13f944d94a7"
      },
      "id": "uKDvlS-ut2iJ",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joseph P. Kennedy and his wife Rose Kennedy had nine children. Their children were Joseph Jr., John F. (Jack), Rosemary, Kathleen, Eunice, Patricia, Robert F. (Bobby), Jean, and Edward M. (Ted).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpCjnMfjt6xS"
      },
      "id": "bpCjnMfjt6xS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative LLM"
      ],
      "metadata": {
        "id": "Lz1mrbI5t-kF"
      },
      "id": "Lz1mrbI5t-kF"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import userdata (where you stored your Colab secret)\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 2. Get your API key from Colab secrets\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# 3. Set it as environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "\n",
        "# 4. Import and create ChatGroq LLM\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llamaChatModel = ChatGroq(\n",
        "    model=\"llama3-70b-8192\",   # you can also try \"llama3-8b-8192\" for cheaper runs\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# 5. Call the model\n",
        "response = llamaChatModel.invoke(\"Tell me a fun fact about space.\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeoUcHKIuD4K",
        "outputId": "508a6ef7-8391-439b-e43c-f349a3fb345f"
      },
      "id": "PeoUcHKIuD4K",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's one:\n",
            "\n",
            "**There is a giant storm on Jupiter that has been raging for at least 187 years!**\n",
            "\n",
            "The Great Red Spot, as it's called, is a persistent anticyclonic storm on Jupiter, which means that it's a high-pressure region with clockwise rotation. It's so large that three Earths could fit inside it.\n",
            "\n",
            "The storm was first observed in 1831, and it's been continuously monitored since then. Despite its incredible longevity, the Great Red Spot is actually shrinking, and its color has changed from a deep red to more of a pale pink over the years.\n",
            "\n",
            "Isn't that just mind-blowing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
        "    (\"human\", \"How many members of the family died tragically?\"),\n",
        "]"
      ],
      "metadata": {
        "id": "yy5_0RzjuZPy"
      },
      "id": "yy5_0RzjuZPy",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llamaChatModel.invoke(messages)"
      ],
      "metadata": {
        "id": "Lmz53JmNuqNh"
      },
      "id": "Lmz53JmNuqNh",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "A68Z1ZC1uwI-",
        "outputId": "4a8c9e20-ac02-479e-bf9a-2d066c18b226"
      },
      "id": "A68Z1ZC1uwI-",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Kennedy family has indeed been marked by a series of tragic events and untimely deaths. Here\\'s a list of some of the most notable ones:\\n\\n1. Joseph P. Kennedy Jr. (1915-1944): The eldest son of Joseph P. Kennedy Sr. and Rose Kennedy, Joe Jr. was a naval aviator who died in action during World War II. He was just 29 years old when his plane exploded on a secret mission over England.\\n\\n2. Kathleen Kennedy Cavendish (1920-1948): Known as \"Kick,\" Kathleen was the sister of John F. Kennedy and the wife of William Cavendish, Marquess of Hartington. She died in a plane crash in France at the age of 28.\\n\\n3. John F. Kennedy (1917-1963): The 35th President of the United States, JFK was assassinated in Dallas, Texas, on November 22, 1963, at the age of 46.\\n\\n4. Robert F. Kennedy (1925-1968): The younger brother of JFK and a U.S. Senator from New York, RFK was assassinated on June 5, 1968, during his presidential campaign in Los Angeles, California. He was 42 years old.\\n\\n5. David Kennedy (1955-1984): The son of Robert F. Kennedy, David died of a drug overdose in a hotel room in Palm Beach, Florida, at the age of 28.\\n\\n6. Michael LeMoyne Kennedy (1958-1997): Another son of Robert F. Kennedy, Michael died in a skiing accident in Aspen, Colorado, on December 31, 1997, at the age of 39.\\n\\n7. John F. Kennedy Jr. (1960-1999): The son of JFK and Jacqueline Kennedy Onassis, John Jr. died in a plane crash off the coast of Massachusetts on July 18, 1999, at the age of 38. His wife Carolyn Bessette Kennedy and her sister Lauren Bessette also perished in the crash.\\n\\n8. Kara Kennedy (1960-2011): The daughter of Ted Kennedy, Kara died of a heart attack while exercising at a Washington, D.C. gym on September 16, 2011, at the age of 51.\\n\\nThese tragic events have led to the notion of a \"Kennedy curse,\" although it\\'s worth noting that the family has also experienced many triumphs and achievements throughout their history.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for completions model\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} story about {topic}\"\n",
        ")\n",
        "\n",
        "llmModelPrompt = prompt_template.format(\n",
        "    adjective=\"curious\",\n",
        "    topic=\"Tesla\"\n",
        ")\n",
        "\n",
        "res = llamaChatModel.invoke(llmModelPrompt)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om7cGVmxuxOK",
        "outputId": "4249b942-31d1-4819-8e36-faf626ed909c"
      },
      "id": "Om7cGVmxuxOK",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a curious story about Nikola Tesla:\n",
            "\n",
            "**The Mysterious Death of Nikola Tesla's Pigeon**\n",
            "\n",
            "Nikola Tesla, the brilliant inventor and electrical engineer, was known for his groundbreaking work on alternating current (AC) systems, X-ray technology, and wireless power transmission. However, few people know about his unusual fascination with pigeons, particularly one special bird that played a significant role in his life.\n",
            "\n",
            "In the 1920s, Tesla lived in New York City, where he would often take long walks in Bryant Park, near his laboratory. During these strolls, he would feed and observe the pigeons, finding solace in their company. Among the flock, one pigeon in particular caught his attention – a beautiful white pigeon with gray tips on its wings.\n",
            "\n",
            "Tesla became enamored with this pigeon, which he named \"Snowball.\" He believed that Snowball possessed a special intelligence and affection for him, and he would often spend hours with the bird, feeding it, and even bringing it into his laboratory.\n",
            "\n",
            "The story takes a strange turn when Tesla began to claim that Snowball was more than just a pet – he believed that the pigeon was a messenger from the spiritual realm. He would often talk to Snowball, and even claimed that the bird would respond to his thoughts and emotions.\n",
            "\n",
            "One day, in 1922, Snowball fell ill and eventually died in Tesla's arms. The inventor was devastated, and his grief was so intense that he claimed to have experienced a kind of spiritual connection with the pigeon's spirit.\n",
            "\n",
            "Tesla believed that Snowball's death was not just a natural occurrence, but rather a deliberate act of sacrifice by the pigeon to help him unlock the secrets of the universe. He wrote in his journal: \"Yes, I loved that pigeon, I loved her as a man loves a woman, and she loved me. When she was ill, I knew it, and I knew she was dying. And I sat there, and I talked to her, and I told her not to worry, that I would take care of her, and she would be all right.\"\n",
            "\n",
            "Tesla's obsession with Snowball's death led him to make some remarkable claims. He believed that the pigeon's spirit had merged with his own, granting him access to higher states of consciousness and allowing him to tap into the fundamental forces of nature.\n",
            "\n",
            "While this story may seem bizarre, it highlights the complex and enigmatic personality of Nikola Tesla, a man who blurred the lines between science, spirituality, and mysticism. Even today, the legend of Tesla's pigeon remains a fascinating footnote in the life of one of the most brilliant minds in history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chat completion model\n"
      ],
      "metadata": {
        "id": "gIfre-n2waKq"
      },
      "id": "gIfre-n2waKq"
    },
    {
      "cell_type": "code",
      "source": [
        "# chat completion model\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an {profession} expert on {topic} give outputs as only bullets points.\"),\n",
        "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question? and their names\"),\n",
        "        (\"ai\", \"Sure!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(\n",
        "    profession=\"Historian\",\n",
        "    topic=\"The Kennedy family\",\n",
        "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
        ")\n",
        "\n",
        "response = llamaChatModel.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ekbiXgwYch",
        "outputId": "8c19109a-029c-40e3-d30d-40994395c58d"
      },
      "id": "V_ekbiXgwYch",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• 29 grandchildren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Prompting"
      ],
      "metadata": {
        "id": "pOWz2sHzxJqd"
      },
      "id": "pOWz2sHzxJqd"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"hello!\", \"output\": \"vanakkam\"},\n",
        "    {\"input\": \"ok\", \"output\": \"sari..\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an English to Tamil translator.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "EvSAxB2TxH7A"
      },
      "id": "EvSAxB2TxH7A",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = llamaChatModel.invoke(\n",
        "    final_prompt.format_messages(\n",
        "        input=\"hello?\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "8eMdEeH2xpm8"
      },
      "id": "8eMdEeH2xpm8",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3cjy-7aLxu0Q",
        "outputId": "dc6387a8-33af-40f3-c4bd-97082f469f1b"
      },
      "id": "3cjy-7aLxu0Q",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'வணக்கம்?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chains\n",
        "A chain is just a sequence of LCEL runnables connected with |, forming a workflow from input → LLM → output."
      ],
      "metadata": {
        "id": "4a_sLuifx-X1"
      },
      "id": "4a_sLuifx-X1"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"hello!\", \"output\": \"vanakkam\"},\n",
        "    {\"input\": \"ok\", \"output\": \"sari..\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an English to Tamil translator.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "chain = final_prompt | llamaChatModel\n",
        "\n",
        "res = chain.invoke({\"input\": \"How are you?\"})\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHmUbIg5xwKz",
        "outputId": "ff2aabaf-4004-4116-df30-e8ca1ae10f6b"
      },
      "id": "MHmUbIg5xwKz",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "நான் நன்றாக இருக்கிறேன் (Nan nandraga irukiren)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llamaChatModel.stream(final_prompt.invoke({\"input\": \"Good morning\"})):\n",
        "    print(chunk.content or \"\", end=\"\", flush=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLTc2-4AypF2",
        "outputId": "d6b0d3d5-339c-451e-b455-04ec55053b73"
      },
      "id": "jLTc2-4AypF2",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "காலை வணக்கம் (Kaalaai Vanakkam)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Expressions Language\n",
        "\n",
        "LangChain Expression Language (LCEL) is a simple way to build pipelines where you connect prompts, models, and output parsers using the | operator\n"
      ],
      "metadata": {
        "id": "jdK_HUNI0M2J"
      },
      "id": "jdK_HUNI0M2J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers\n",
        "\n",
        "In LangChain, Output Parsers are utilities that convert raw LLM text into structured Python objects (e.g., strings, JSON, lists, Pydantic models)."
      ],
      "metadata": {
        "id": "CfFSeiHq0Ziq"
      },
      "id": "CfFSeiHq0Ziq"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "json_prompt = PromptTemplate.from_template(\n",
        "    \"Return a JSON object with an `Sum` key that answers the following question: {question}\"\n",
        ")\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "json_chain = json_prompt | llamaChatModel | json_parser"
      ],
      "metadata": {
        "id": "55D3yQrG0ObV"
      },
      "id": "55D3yQrG0ObV",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_parser.get_format_instructions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HkOaxXMR08Z8",
        "outputId": "3ac2a2db-c753-4e11-f252-5fc5a5ed2285"
      },
      "id": "HkOaxXMR08Z8",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Return a JSON object.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res  = json_chain.invoke({\"question\": \"9+7=?\"})\n"
      ],
      "metadata": {
        "id": "U-5n4IZ91Lic"
      },
      "id": "U-5n4IZ91Lic",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u6rrPKQ1XsE",
        "outputId": "68ddcf22-7b12-450b-c15f-5571c5e6e0b8"
      },
      "id": "7u6rrPKQ1XsE",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sum': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abDEuMh1YFc",
        "outputId": "4a70c354-61a4-4b37-cc49-b1a952ff86a8"
      },
      "id": "3abDEuMh1YFc",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optionally, you can use Pydantic to define a custom output format"
      ],
      "metadata": {
        "id": "PZDBvvqW2hLA"
      },
      "id": "PZDBvvqW2hLA"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field"
      ],
      "metadata": {
        "id": "Q_z29ETF2Ow1"
      },
      "id": "Q_z29ETF2Ow1",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Pydantic Object with the desired output format.\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")"
      ],
      "metadata": {
        "id": "0AaaZsdD2jvP"
      },
      "id": "0AaaZsdD2jvP",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parser referring the Pydantic Object\n",
        "parser = JsonOutputParser(pydantic_object=Joke)\n",
        "\n",
        "# Add the parser format instructions in the prompt definition.\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Create a chain with the prompt and the parser\n",
        "chain = prompt | chatModel | parser\n",
        "\n",
        "chain.invoke({\"query\": \"Tell me a IT joke.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUmoLv_s2luZ",
        "outputId": "c70e3e1f-bdd2-473c-a5c3-fe6c7376cd4b"
      },
      "id": "aUmoLv_s2luZ",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why do programmers prefer dark mode?',\n",
              " 'punchline': 'Because light attracts bugs!'}"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H280a-Yy2naC"
      },
      "id": "H280a-Yy2naC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "rCNYmHaVAjXX"
      },
      "id": "rCNYmHaVAjXX"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import userdata (where you stored your Colab secret)\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 2. Get your API key from Colab secrets\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# 3. Set it as environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "\n",
        "# 4. Import and create ChatGroq LLM\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llamaChatModel = ChatGroq(\n",
        "    model=\"llama3-70b-8192\",   # you can also try \"llama3-8b-8192\" for cheaper runs\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# 5. Call the model\n",
        "response = llamaChatModel.invoke(\"Tell me a fun fact about space.\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQGnz8xhAkG2",
        "outputId": "45b3ab2f-c8e6-434b-cc10-878edf3e737d"
      },
      "id": "UQGnz8xhAkG2",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's one:\n",
            "\n",
            "**There is a giant storm on Jupiter that has been raging for at least 187 years!**\n",
            "\n",
            "The Great Red Spot, as it's called, is a persistent anticyclonic storm on Jupiter, which means that it's a high-pressure region with clockwise rotation. It's so large that three Earths could fit inside it.\n",
            "\n",
            "The storm was first observed in 1831, and it's been continuously monitored since then. Despite its incredible longevity, the Great Red Spot is actually shrinking, and its color has changed from a deep red to more of a pale pink over the years.\n",
            "\n",
            "Isn't that just mind-blowing?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}