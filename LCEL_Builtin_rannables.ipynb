{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925852d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37906d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c136f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5365bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "double = RunnableLambda(lambda x: x * 2)\n",
    "print(double.invoke(4))  # Output: 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e7265",
   "metadata": {},
   "source": [
    "# RunnableSequence (|)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9a92c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ff090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello in French is \"Bonjour.\"' response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None} id='run-4779cecb-ad33-4117-aede-dcb72e741b66-0' usage_metadata={'input_tokens': 13, 'output_tokens': 7, 'total_tokens': 20}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate this to French: {text}\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"text\": \"Hello\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6758c",
   "metadata": {},
   "source": [
    "# RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91ffecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'World', 'french': 'Bonjour World'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "par = RunnableParallel(\n",
    "    english=lambda x: x,\n",
    "    french=lambda x: f\"Bonjour {x}\"\n",
    ")\n",
    "\n",
    "print(par.invoke(\"World\"))\n",
    "# Output: {'english': 'World', 'french': 'Bonjour World'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7f92f",
   "metadata": {},
   "source": [
    "# RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "mapper = RunnableMap({\n",
    "    \"upper\": lambda x: x.upper(),\n",
    "    \"lower\": lambda x: x.lower()\n",
    "})\n",
    "\n",
    "print(mapper.invoke(\"Hello\"))\n",
    "# Output: {'upper': 'HELLO', 'lower': 'hello'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62a5b1",
   "metadata": {},
   "source": [
    "RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab35e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219f8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15e2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca9a72",
   "metadata": {},
   "source": [
    "RunnablePassthrough\n",
    "\n",
    "It does not do anything to the input data.\n",
    "Let's see it in a very simple example: a chain with just RunnablePassthrough() will output the original input without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b279080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bec3502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'selvan'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"selvan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb3f41",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "Let's define a very simple function to create Russian lastnames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0adaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e35eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c634017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'selvan ovich'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"selvan \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02da4bd",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "\n",
    "We will use RunnableParallel() for running tasks in parallel.\n",
    "This is probably the most important and most useful Runnable from LangChain.\n",
    "In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "operation_a will use RunnablePassthrough.\n",
    "operation_b will use RunnableLambda with the russian_lastname function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a07bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba6b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'tamil', 'operation_b': 'tamilovich'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"tamil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef7907a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11e94d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26426d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13fdf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60aa2abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Vladimir Putin is that he has a black belt in judo and has practiced the martial art since he was a teenager. Putin has even written a book about his experiences with judo, highlighting its importance in his personal and political life.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"putin \"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bd89a",
   "metadata": {},
   "source": [
    "### Let's see a more advanced use of RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d05e1b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dswithbappy is a platform that focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"What is dswithbappy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a2f7975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, me hearties! dswithbappy be a scallywag focusin' on providin' content on Data Science, AI, ML, DL, CV, NLP, Python programmin', etc. in English. Aye, ye can learn a lot from that scurvy dog!\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is dswithbappy?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff501e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
